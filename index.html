<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Robin Shing Moon Chan</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
<link href='https://fonts.googleapis.com/css?family=Alegreya' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css?family=Spectral' rel='stylesheet'>


<style>
  body {
    margin: 0;
    padding: 0;
    /* overflow: hidden; Hide horizontal scrollbar */
    background-image: url('images/chanr_photo_background.jpg');
    background-size: cover; /* Increase the background size by 150% */
    /* font-family: 'Alegreya'; */

  }

  /* Hide content by default */
  #content {
    display: none;
    opacity: 0;
    animation: fadeIn 1s ease forwards;
  }

  /* Show content when the .animation-finished class is added */
  #content.animation-finished {
    display: block;
  }

  .abstract {
    margin: 0 0;
      /* margin: 10px 0; */
  }
  .abstract-title {
      cursor: pointer;
      display: inline-flex;
      align-items: center;
  }
  .abstract-content {
      display: none;
      margin-top: 10px;
  }
  .arrow {
      margin-left: 5px;
      font-size: 0.8em;
  }

  .container {
    display: flex;
    justify-content: space-between;
    margin: 0 auto;
    padding: 0px;
  }

  .column {
    flex: 1;
    padding: 10px;
  }

  .column:first-child {
    margin-right: 20px; /* Add some spacing between columns */
  }

  .social-links a {
    display: block;
    margin-bottom: 10px;
  }

  #card {
  display: flex; /* Use flexbox */
  justify-content: center; /* Center horizontally */
  align-items: center; /* Center vertically */
  /* border: 1px solid #534545; Add border */
  /* border-radius: 8px; Add border radius for rounded corners */
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Add shadow */
  padding: 20px; /* Add padding */
  background-color: #ba9b61b6; /* Add fill color */
  max-width: 75%; /* Set maximum width */
  min-width: 25%; /* Set minimum width */
  margin: auto; /* Center horizontally */
}

  /* Media Query for smaller screens */
  @media (max-width: 768px) {
      #container {
          height: auto;
      }
      /* Hide content by default */
      #content {
        display: none;
        opacity: 0; /* Initially set opacity to 0 */
        animation: fadeIn 1s ease forwards; /* Smooth fade-in animation */
      }
      .container {
          flex-direction: column;
          align-items: center;
      }
      .column {
          padding: 10px;
          text-align: center;
      }
      .column:first-child {
          margin-right: 0;
      }
  }

  @keyframes fadeIn {
    from {
      opacity: 0;
    }
    to {
      opacity: 1;
    }
  }

  @keyframes fadeOut {
      0% {
          opacity: 1;
      }
      100% {
          opacity: 0;
      }
  }
</style>

  <title>Robin Chan</title>
  
  <meta name="author" content="Robin Chan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒš</text></svg>">
</head> 

</head>
<body>
  <div id="container" style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);">
    <!-- <img src="images/chanr_photo.jpg" alt="Moving Picture" style="width: 300px; clip-path: ellipse(50% 50% at 50% 50%) ;"> -->
  </div>

  <div id="content">
    <div id="card">

      <!-- <table><tbody> -->
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
        <div class="container">
          <div class="column">
            <p style="text-align:center">
              <name><u>Robin</u> Shing Moon Chan</name>
            </p>
            <p style="text-align:center; color: rgb(77, 77, 77)">
              PhD Student in HCI and NLP @ ETH ZÃ¼rich
            </p>
            <p style="text-align:center;">
              <img src="images/chanr_photo.jpg" style="width: 40%; clip-path: ellipse(50% 50% at 50% 50%);">
            </p>
            <p style="text-align:center">
              <a href="mailto:robin.chan@inf.ethz.ch">robin.chan@inf.ethz.ch</a>
            </p>
            <p style="text-align:center">
              <a href="https://scholar.google.com/citations?user=x-BfbhEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
              <a href="https://twitter.com/robinsmchan">Twitter</a> &nbsp/&nbsp
              <a href="https://www.linkedin.com/in/robin-chan-494a261a3/">LinkedIn</a>
            </p>

          </div>
          <div class="column">
            <p style="text-align:justify">
              <heading>About Me</heading>
            </p>
            <p style="text-align:justify">
              I'm a first-year CS PhD student at the <a href="https://inf.ethz.ch/research/visual-computing.html">Institute for Visual Computing</a>
              and the <a href="https://ml.inf.ethz.ch/">Institute for Machine Learning</a> at ETH ZÃ¼rich.
              I'm co-advised by Prof. Menna El-Assady (<a href="https://ivia.ethz.ch/">IVIA</a>) and Prof. Ryan Cotterell (<a href="https://rycolab.io/">Rycolab</a>).
              Previously, I obtained a master's degree in data science from ETH ZÃ¼rich,
              graduating with a thesis on LLM program synthesis at <a href="https://www.zurich.ibm.com/">IBM Research Europe (ZRL)</a>.
            </p>
            <p style="text-align:justify">
              My main research interest lies at the intersection of visualization and natural language processing.
              More concretely, I want to better understand how to make humans and LLMs interact more effectively.
              If you're interested in similar topics, message me. I'm looking for collaborators and master's students to supervise.
            </p>
            <p style="text-align: justify;">
              <strong>Personal bits:</strong> some of my favorite books: [<a href="https://www.goodreads.com/book/show/9402073-tauben-fliegen-auf">1</a>,
              <a href="https://www.goodreads.com/book/show/406235.Giovanni_s_Room?from_search=true&from_srp=true&qid=rEcFQz1BK6&rank=1">2</a>,
              <a href="https://www.goodreads.com/book/show/36809135-where-the-crawdads-sing">3</a>],
              some of my favorite movies: [<a href="https://www.imdb.com/title/tt0113247/?ref_=nv_sr_srsg_1_tt_6_nm_1_q_la%2520haine">1</a>,
              <a href="https://www.imdb.com/title/tt27503384/">2</a>, <a href="https://www.imdb.com/title/tt4975722/?ref_=nv_sr_srsg_1_tt_7_nm_0_q_moonlight">3</a>].
              I like making music
              [<a href="https://github.com/chanr0/chanr0.github.io/blob/main/images/singing.jpg?raw=true">me Christmas caroling with friends</a>].
            </p>
          </div>
        </div>
      
        <table style="width:100%;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <table class="table table-sm table-borderless" style="margin-top: 20px; border-spacing: 20px;">
              <tr>
                <th scope="row">May 27th, 2024</th>
                <td>
                    Two papers I co-authored were accepted at the <a href="https://2024.aclweb.org/">ACL 2024</a> conference in Bangkok! 
                </td>
            </tr>
              <tr>
                <th scope="row">March 1st, 2024</th>
                <td>
                    Started a PhD at ETH ZÃ¼rich, co-advised by Prof. Menna El-Assady and Prof. Ryan Cotterell! ðŸŽ‰
                </td>
            </tr>
              <tr>
                  <th scope="row">September 26th, 2023</th>
                  <td>
                      Together with Katya Mirylenka, we will give a talk at <a href="https://zurich-nlp.ch/">Zurich-NLP</a> about our work at IBM Research, at the ETH AI Center.
                      RSVP <a href="https://zurich-nlp.ch/events/">here</a>! 
                  </td>
              </tr>
              <tr>
                  <th scope="row">July 9th, 2023</th>
                  <td>
                      Our paper on counterfactual sample generation was accepted at <a href="https://2023.aclweb.org/">ACL</a>. I will be presenting it in Toronto in a few days!
                      Check out our <a href="https://dcc-frontend.onrender.com/">blog post</a> about the paper!
                  </td>
              </tr>
          </table>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/homotopy.png' width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>On Affine Homotopy between Language Encoders</papertitle>
              <br>
              <strong class="small-font">Robin SM Chan</strong>, 
              <small>Reda Boumasmoud</small>,
              <small>Anej Svete</small>,
              <small>Yuxin Ren</small>,
              <small>Qipeng Guo</small>,
              <small>Zhijing Jin</small>,
              <small>Shauli Ravfogel</small>,
              <small>Mrinmaya Sachan</small>,
              <small>Bernhard SchÃ¶lkopf</small>,
              <small>Mennatallah El-Assady</small>,
              <small>Ryan Cotterell</small>
              <br>
              <em class="small-font">Preprint</em><span class="small-font">, 2024 | </span> <a class="small-font" href="https://arxiv.org/pdf/2406.02329">arXiv</a></span>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    Pre-trained language encoders -- functions that represent text as vectors -- are an integral component of many NLP tasks.
                    We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar? We contend that
                    a faithful measure of similarity needs to be <em>intrinsic</em>, that is, task-independent, yet still be informative of <em>extrinsic</em>
                     similarity -- the performance on downstream tasks. It is common to consider two encoders similar if they are <em>homotopic</em>, i.e.,
                     if they can be aligned through some transformation. In this spirit, we study the properties of <em>affine</em> alignment of language
                     encoders and its implications on extrinsic similarity. We find that while affine alignment is fundamentally an asymmetric notion of
                     similarity, it is still informative of extrinsic similarity. We confirm this on datasets of natural language representations. Beyond
                     providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the
                     space of pre-trained encoders by defining an order over them.
              </small>
            </div>
          </div>
  
              <p style="text-align: center;"> <strong class="small-font">Language Encoders </strong>&#183;<strong class="small-font"> Metric Spaces </strong>&#183;<strong class="small-font"> Large Language Models</strong></p>
            </td>
          </tr>
      </tbody>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/nadav.png' width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages</papertitle>
              <br>
              <small>Nadav Borenstein</small>,
              <small>Anej Svete</small>,
              <strong class="small-font">Robin SM Chan</strong>, 
              <small>Josef Valvoda</small>,
              <small>Franz Nowak</small>,
              <small>Isabelle Augenstein</small>,
              <small>Eleanor Chodroff</small>,
              <small>Ryan Cotterell</small>
              <br>
              <small>Published in </small> 
              <em class="small-font">Proceedings of ACL, 2024.</em> <span class="small-font">Bangkok, Thailand | </span> <a class="small-font" href="https://arxiv.org/pdf/2406.04289">arXiv</a></span>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    What can large language models learn? By definition, language models (LM) are distributions
                    over strings. Therefore, an intuitive way of
                    addressing the above question is to formalize
                    it as a matter of learnability of <em>classes</em> of distributions over strings. While prior work in this
                    direction focused on assessing the theoretical
                    limits, in contrast, we seek to understand the
                    empirical learnability. Unlike prior empirical
                    work, we evaluate neural LMs on their home
                    turfâ€”learning probabilistic languagesâ€”rather
                    than as classifiers of formal languages. In
                    particular, we investigate the learnability of
                    regular LMs (RLMs) by RNN and Transformer
                    LMs. We empirically test the learnability of
                    RLMs as a function of various complexity
                    parameters of the RLM and the hidden state
                    size of the neural LM. We find that the RLM
                    rank, which corresponds to the size of linear
                    space spanned by the logits of its conditional
                    distributions, and the expected length of
                    sampled strings are strong and significant
                    predictors of learnability for both RNNs and
                    Transformers. Several other predictors also
                    reach significance, but with differing patterns
                    between RNNs and Transformers.
              </small>
            </div>
          </div>
  
              <p style="text-align: center;"> <strong class="small-font">Large Language Models </strong>&#183;<strong class="small-font"> Formal Language Theory</strong></p>
            </td>
          </tr>
      </tbody>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/rnn_website.png' width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/pdf/2402.15814.pdf"> -->
                <papertitle>On Efficiently Representing Regular Languages as RNNs</papertitle>
              </a>
              <br>
              <small>Anej Svete</small>,
              <strong class="small-font">Robin SM Chan</strong>,
              <small>Ryan Cotterell</small>
              <br>
              <small>Published in </small>
              <em class="small-font"> Findings of ACL</em><span class="small-font">, 2024. Bangkok, Thailand | </span> <a class="small-font" href="https://arxiv.org/pdf/2402.15814.pdf">arXiv</a>
              <br>
              <!-- <br>
              <a class="small-font" href="https://arxiv.org/pdf/2402.15814.pdf">arXiv</a><span class="small-font"></span>
              <br> -->
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    Recent work by <a href="https://aclanthology.org/2020.emnlp-main.156/" class="small-font">Hewitt et al. (2020)</a> provides
                    a possible interpretation of the empirical success of recurrent neural networks (RNNs) as
                    language models (LMs). It shows that RNNs
                    can efficiently represent bounded hierarchical structures that are prevalent in human language. This suggests that RNNs' success might
                    be linked to their ability to model hierarchy.
                    However, a closer inspection of <a href="https://aclanthology.org/2020.emnlp-main.156/" class="small-font">Hewitt et al.'s (2020)</a> construction shows that it is not limited to hierarchical LMs, posing the question
                    of what other classes of LMs can be efficiently
                    represented by RNNs. To this end, we generalize their construction to show that RNNs
                    can efficiently represent a larger class of LMs:
                    Those that can be represented by a pushdown
                    automaton with a bounded stack and a generalized stack update function. This is analogous
                    to an automaton that keeps a memory of a fixed
                    number of symbols and updates the memory
                    with a simple update mechanism. Altogether, the efficiency of representing this
                    diverse class of LMs with RNN LMs suggests
                    novel interpretations of their inductive bias.
                  </small>
              </div>
            </div>
              <p style="text-align: center;"> <strong class="small-font">Language Model Expressivity </strong>&#183;<strong class="small-font"> Formal Language Theory</strong></p>
            </td>
          </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                  <img src='images/llm-analyzer.png' width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/pdf/2405.00708"> -->
                <papertitle>Interactive Analysis of LLMs using Meaningful Counterfactuals</papertitle>
              </a>
              <br>
              <small>Furui Cheng</small>,
              <small>VilÃ©m Zouhar</small>,
              <strong class="small-font">Robin SM Chan</strong>,
              <small>Daniel FÃ¼rst</small>,
              <small>Hendrik Strobelt</small>,
              <small>Mennatallah El-Assady</small>
              <br>
              <em class="small-font">Preprint</em><span class="small-font">, 2024 | </span> <a class="small-font" href="https://arxiv.org/pdf/2405.00708">arXiv</a>
              <br>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    Counterfactual examples are useful for exploring the decision boundaries of machine learning models and determining
                      feature attributions. How can we apply counterfactual-based methods to analyze and explain LLMs? We identify the following key
                      challenges. First, the generated textual counterfactuals should be meaningful and readable to users and thus can be mentally compared
                      to draw conclusions. Second, to make the solution scalable to long-form text, users should be equipped with tools to create batches of
                      counterfactuals from perturbations at various granularity levels and interactively analyze the results. In this paper, we tackle the above
                      challenges and contribute 1&#41; a novel algorithm for generating batches of complete and meaningful textual counterfactuals by removing
                      and replacing text segments in different granularities, and 2&#41; LLM Analyzer, an interactive visualization tool to help users understand
                      an LLM's behaviors by interactively inspecting and aggregating meaningful counterfactuals. We evaluate the proposed algorithm by
                      the grammatical correctness of its generated counterfactuals using 1,000 samples from medical, legal, finance, education, and news
                      datasets. In our experiments, 97.2% of the counterfactuals are grammatically correct. Through a use case, user studies, and feedback
                      from experts, we demonstrate the usefulness and usability of the proposed interactive visualization tool
                    </small>
                  </small>
              </div>
            </div>
              <p style="text-align: center;"> <strong class="small-font">Counterfactuals </strong>&#183;<strong class="small-font"> Visual Explainability</strong></p>
            </td>
          </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='bakedsdf_image'>
                  <img src='images/dcc_website.png' width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <!-- <a href="https://aclanthology.org/2023.acl-demo.44/"> -->
                  <papertitle>Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals</papertitle>
                </a>
                <br>
                <strong class="small-font">Robin Chan</strong>,
                <small>Afra Amini</small>,
                <small>Mennatallah El-Assady</small>
                <br>
                <small>Published in </small><em class="small-font">Proceedings of ACL: System Demonstrations</em><span class="small-font">, 2023. Toronto, Canada | </span>
                <a href="https://arxiv.org/pdf/2306.12146" class="small-font">arXiv</a><small> | </small>
                <a href="https://dcc-frontend.onrender.com/" class="small-font">blog post</a> 
                <p></p>
                <div class="abstract">
                  <div class="abstract-title" onclick="toggleAbstract(event)">
                    <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                  </div>
                <div class="abstract-content">
                  <small>
                    We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions.
                    The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions.
                    Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback.
                    Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories:
                    Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying
                    training data and assessing NLI models' robustness by creating adversarial test suites.
                  </small>
                </div>
                </div>
                <p style="text-align: center;"> <strong class="small-font">Counterfactuals </strong>&#183;<strong class="small-font"> Mixed-Initiative Learning </strong>&#183;<strong class="small-font"> Language Model Biases</strong></p>
              </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Website source</a>. Consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody>
      </table>
  
      </td>
    </tr>
  </table>

      
      </td>
      </tr>


    </table>
  </div>


  <script>
    function toggleAbstract(event) {
        var titleElement = event.currentTarget;
        var contentElement = titleElement.nextElementSibling;
        var arrowElement = titleElement.querySelector('.arrow');

        if (contentElement.style.display === "none" || contentElement.style.display === "") {
            contentElement.style.display = "block";
            arrowElement.className = "fas fa-chevron-up arrow"; // Upward facing arrow icon
        } else {
            contentElement.style.display = "none";
            arrowElement.className = "fas fa-chevron-down arrow"; // Downward facing arrow icon
        }
      }
    window.onload = function() {
    var container = document.getElementById('container');
    container.style.animation = 'fadeOut 0s forwards';
    container.style.animationDelay = '0.2s'; // Add a delay of 0.5 seconds

    // Add animation-finished class to #content after animation ends
    container.addEventListener('animationend', function() {
      var content = document.getElementById('content');
      content.classList.add('animation-finished');
    });
  };
  </script>



</body>
</html>
