<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Robin Shing Moon Chan</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
<link href='https://fonts.googleapis.com/css?family=Alegreya' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css?family=Spectral' rel='stylesheet'>


<style>
  body {
    margin: 0;
    padding: 0;
    background-color: #687f97;
}

  /* Hide content by default */
  #content {
    display: none;
    opacity: 0;
    animation: fadeIn 1s ease forwards;
  }

  /* Show content when the .animation-finished class is added */
  #content.animation-finished {
    display: block;
  }

  .abstract {
    margin: 0 0;
      /* margin: 10px 0; */
  }
  .abstract-title {
      cursor: pointer;
      display: inline-flex;
      align-items: center;
  }
  .abstract-content {
      display: none;
      margin-top: 10px;
  }
  .arrow {
      margin-left: 5px;
      font-size: 0.8em;
  }

  .container {
    display: flex;
    justify-content: space-between;
    margin: 0 auto;
    padding: 0px;
  }

  .column {
    flex: 1;
    padding: 10px;
  }

  .column:first-child {
    margin-right: 20px; /* Add some spacing between columns */
  }

  .social-links a {
    display: block;
    margin-bottom: 10px;
  }

  #card {
  display: flex; /* Use flexbox */
  justify-content: center; /* Center horizontally */
  align-items: center; /* Center vertically */
  /* border: 1px solid #534545; Add border */
  /* border-radius: 8px; Add border radius for rounded corners */
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Add shadow */
  padding: 20px; /* Add padding */
  background-color: #d3d3d3; /* Add fill color */
  max-width: 75%; /* Set maximum width */
  min-width: 25%; /* Set minimum width */
  margin: auto; /* Center horizontally */
}

  /* Media Query for smaller screens */
  @media (max-width: 768px) {
      #container {
          height: auto;
      }
      /* Hide content by default */
      #content {
        display: none;
        opacity: 0; /* Initially set opacity to 0 */
        animation: fadeIn 1s ease forwards; /* Smooth fade-in animation */
      }
      .container {
          flex-direction: column;
          align-items: center;
      }
      .column {
          padding: 10px;
          text-align: center;
      }
      .column:first-child {
          margin-right: 0;
      }
  }

  @keyframes fadeIn {
    from {
      opacity: 0;
    }
    to {
      opacity: 1;
    }
  }

  @keyframes fadeOut {
      0% {
          opacity: 1;
      }
      100% {
          opacity: 0;
      }
  }
</style>

  <title>Robin Chan</title>
  
  <meta name="author" content="Robin Chan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒš</text></svg>">
</head> 

</head>
<body>
  <div id="container" style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);">
    <!-- <img src="images/chanr_photo.jpg" alt="Moving Picture" style="width: 300px; clip-path: ellipse(50% 50% at 50% 50%) ;"> -->
  </div>

  <div id="content">
    <div id="card">

      <!-- <table><tbody> -->
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
        <div class="container">
          <div class="column">
            <p style="text-align:center">
              <name><u>Robin</u> Shing Moon Chan</name>
            </p>
            <p style="text-align:center; color: rgb(77, 77, 77)">
              PhD Student in HCI and NLP @ ETH ZÃ¼rich
            </p>
            <p style="text-align:center;">
              <img src="images/chanr.jpeg" style="width: 150px; height: 160px; border-radius: 50%; object-fit: cover;">
            </p>
            <p style="text-align:center">
              <a href="mailto:robin.chan@inf.ethz.ch">robin.chan@inf.ethz.ch</a>
            </p>
            <p style="text-align:center">
              <a href="https://scholar.google.com/citations?user=x-BfbhEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
              <a href="https://bsky.app/profile/robinsmchan.bsky.social">Bluesky</a> &nbsp/&nbsp
              <a href="https://www.linkedin.com/in/robin-chan-494a261a3/">LinkedIn</a>
            </p>

          </div>
          <div class="column">
            <p style="text-align:justify">
              <heading>About Me</heading>
            </p>
            <p style="text-align:justify">
              I'm a second-year CS PhD student at the <a href="https://inf.ethz.ch/research/visual-computing.html">Institute for Visual Computing</a>
              and the <a href="https://ml.inf.ethz.ch/">Institute for Machine Learning</a> at ETH ZÃ¼rich.
              I'm co-advised by Prof. Menna El-Assady at <a href="https://ivia.ethz.ch/">IVIA</a> and Prof. Ryan Cotterell at <a href="https://rycolab.io/">Rycolab</a>.
              Previously, I obtained a master's degree in data science from ETH ZÃ¼rich,
              graduating with a thesis on LLM code generation at <a href="https://www.zurich.ibm.com/">IBM Research Europe (ZRL)</a>.
            </p>
            <p style="text-align:justify">
              My main research interest lies at the intersection of human-computer interaction and natural language processing.
              More concretely, I want to better understand how to make humans and LLMs interact more effectively.
              If you're interested in similar topics, don't hesitate to message me.
            </p>
            <p style="text-align: justify;">
              <strong>Personal bits:</strong> some of my favorite books: [<a href="https://www.goodreads.com/book/show/9402073-tauben-fliegen-auf">1</a>,
              <a href="https://www.goodreads.com/book/show/406235.Giovanni_s_Room?from_search=true&from_srp=true&qid=rEcFQz1BK6&rank=1">2</a>,
              <a href="https://www.goodreads.com/book/show/36809135-where-the-crawdads-sing">3</a>],
              some of my favorite movies: [<a href="https://www.imdb.com/title/tt0113247/?ref_=nv_sr_srsg_1_tt_6_nm_1_q_la%2520haine">1</a>,
              <a href="https://www.imdb.com/title/tt27503384/">2</a>, <a href="https://www.imdb.com/title/tt4975722/?ref_=nv_sr_srsg_1_tt_7_nm_0_q_moonlight">3</a>].
              I like making music
              [<a href="https://github.com/chanr0/chanr0.github.io/blob/main/images/singing.jpg?raw=true">me Christmas caroling with friends</a>].
            </p>
          </div>
        </div>
      
        <table style="width:100%;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <table class="table table-sm table-borderless" style="margin-top: 20px; border-spacing: 20px;">
              <tr>
                <th scope="row">March 26th, 2025</th>
                <td>
                    Our paper <em>Finding Needles in Document Haystacks: Augmenting Serendipitous Claim Retrieval Workflows
                    </em> was accepted at <a href="https://chi2025.acm.org/">CHI 2025</a> in Yokohama, Japan &mdash;
                    congratulations to the team!
                </td>
              </tr>
              <tr>
                <th scope="row">December 18th, 2024</th>
                <td>
                    Our paper <em>A Design Space for Intelligent Dialogue Augmentation</em> was accepted at <a href="https://iui.acm.org/2025/index.html">ACM IUI 2025</a>!
                    I will be presenting it in Cagliari in March. Thanks to all co-authors!
                </td>
              </tr>
              <tr>
                <th scope="row">October 2nd, 2024</th>
                <td>
                    The paper we wrote at IBM Research on API integration with LLMs was accepted at <a href="https://2024.emnlp.org/">EMNLP 2024</a> (Industry Track)
                    &mdash; thanks to all co-authors!
                    My colleague Thomas Gschwind from IBM Research will be presenting it in Miami in November.
                </td>
              </tr>
                <th scope="row">September 26th, 2024</th>
                <td>
                    Our paper <em>On Affine Homotopy between Language Encoders </em> was accepted at <a href="https://neurips.cc/">NeurIPS 2024</a>.
                    I will be presenting it in Vancouver in December.
                    Thanks to all co-authors!
                </td>
              </tr>
              <tr>
                <th scope="row">August 12th, 2024</th>
                <td>
                    We gave a tutorial about the representational capacity of neural language models at <a href="https://2024.aclweb.org/">ACL 2024</a> in Bangkok. 
                    Check out our <a href="https://acl2024.ivia.ch/">interactive tutorial webpage</a>!
                </td>
              </tr>
              <tr>
                <th scope="row">May 27th, 2024</th>
                <td>
                    Two papers I co-authored were accepted at <a href="https://2024.aclweb.org/">ACL 2024</a>! 
                </td>
              </tr>
              <tr>
                <th scope="row">March 1st, 2024</th>
                <td>
                    Started a PhD at ETH ZÃ¼rich, co-advised by Prof. Menna El-Assady and Prof. Ryan Cotterell! ðŸŽ‰
                </td>
            </tr>
              <tr>
                  <th scope="row">September 26th, 2023</th>
                  <td>
                      Together with Katya Mirylenka, we will give a talk at <a href="https://zurich-nlp.ch/">Zurich-NLP</a> about our work at IBM Research, at the ETH AI Center.
                      RSVP <a href="https://zurich-nlp.ch/events/">here</a>! 
                  </td>
              </tr>
              <tr>
                  <th scope="row">July 9th, 2023</th>
                  <td>
                      Our paper on counterfactual sample generation was accepted at <a href="https://2023.aclweb.org/">ACL</a>. I will be presenting it in Toronto in a few days!
                      Check out our <a href="https://dcc-frontend.onrender.com/">blog post</a> about the paper!
                  </td>
              </tr>
          </table>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/haystacks.png' width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Finding Needles in Document Haystacks: Augmenting Serendipitous Claim Retrieval Workflows</papertitle>
              <br>
              <small>Moritz DÃ¼ck</small>,
              <small>Steffen Holter</small>,
              <u class="small-font">Robin SM Chan</u>, 
              <small>Rita Sevastjanova</small>,
              <small>Menna El-Assady</small>
              <br>
              <em class="small-font">Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI),  </em> <span class="small-font"> 2025 </span>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> 
                </div>
                <div class="abstract-content">
                  <small>
                    Preliminary exploration of vast text corpora for generating and validating hypotheses, typical in academic inquiry, requires flexible navigation and rapid validation of claims.
                    Navigating the corpus by titles, summaries, and abstracts might neglect information, whereas identifying the relevant context-specific claims through in-depth reading is unfeasible with rapidly increasing publication numbers.
                    Our paper identifies three typical user pathways for hypothesis exploration and operationalizes sentence-based retrieval combined with effective contextualization and provenance tracking in a unified workflow.
                    We contribute an interface that augments the previously laborious tasks of claim identification and consistency checking using NLP techniques while balancing user control and serendipity.
                    Use cases, expert interviews, and a user study with 10 participants demonstrate how the proposed workflow enables users to traverse literature corpora in novel and efficient ways.
                    For the evaluation, we instantiate the tool within two independent domains, providing novel insights into the analysis of political discourse and medical research.
                  </small>
                </div>
              </div>
  
              <p style="text-align: center;"> <strong class="small-font">Interactive Systems </strong>&#183;<strong class="small-font"> Human-AI Collaboration </strong>&#183;<strong class="small-font"> Language Models</strong></p>
            </td>
          </tr>


          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/dialogue-augmentation.png' width="100%" class="publication-img" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>A Design Space for Intelligent Dialogue Augmentation</papertitle>
              <br>
              <u class="small-font">Robin SM Chan</u>, 
              <small>Anne Marx</small>,
              <small>Alison Kim</small>,
              <small>Menna El-Assady</small>
              <br>
              <em class="small-font">Proceedings of the 30th International ACM Conference on Intelligent User Interfaces (IUI), </em> <span class="small-font"> 2025 </span>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    The use of intelligent agents in communication is a growing trend aimed at enhancing the efficiency and quality of interactions.
                    As such, <em>dialogue augmentation systems</em> -- text processing systems that interactively enhance ongoing written or spoken communication -- are gaining significant popularity across domains.
                    While technical limitations had previously inhibited their real-time usage for effective communication augmentation, recent developments in language processing have improved their capabilities to contribute to dialogue as intelligent, emancipated, and proactive agents.
                    While other works on dialogue augmentation focus on evaluating design considerations for specific applications of these systems, we lack a unified understanding of the broader design principles that apply to dialogue more generally.
                    Through a literature review and mixed-methods analysis of 78 existing systems, we iteratively define a comprehensive design space for intelligent dialogue augmentation systems.
                    To further ground our analysis, we interweave Clark's models of dialogue with concepts in human-AI collaboration and discuss trends in the evolving role of dialogue augmentation systems along five dimensions -- dialogue context, augmentation context, task, interaction, and model.
                    Based on the identified trends, we discuss concrete challenges for broader adoption, highlighting the need to design <em>trusted</em>, <em>seamless</em>, and <em>timely</em> augmentations.
                    The design space contributes as a mechanism for researchers to facilitate defining design choices during development, situate their systems in the current landscape of works, and understand opportunities for future research.
                  </small>
                </div>
              </div>
  
              <p style="text-align: center;"> <strong class="small-font">Human-AI Collaboration </strong>&#183;<strong class="small-font"> Speech Processing </strong>&#183;<strong class="small-font"> Language Models</strong></p>
            </td>
          </tr>


          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/homotopy.png'  class="publication-img" width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>On Affine Homotopy between Language Encoders</papertitle>
              <br>
              <u class="small-font">Robin SM Chan</u>, 
              <small>Reda Boumasmoud</small>,
              <small>Anej Svete</small>,
              <small>Yuxin Ren</small>,
              <small>Qipeng Guo</small>,
              <small>Zhijing Jin</small>,
              <small>Shauli Ravfogel</small>,
              <small>Mrinmaya Sachan</small>,
              <small>Bernhard SchÃ¶lkopf</small>,
              <small>Menna El-Assady</small>,
              <small>Ryan Cotterell</small>
              <br>
              <em class="small-font">Advances in Neural Information Processing 38 (NeurIPS)</em><span class="small-font">, 2024 | </span> <a class="small-font" href="https://proceedings.neurips.cc/paper_files/paper/2024/file/86040ae1ecc64655bdbdfbbf774ead26-Paper-Conference.pdf">pdf</a></span>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    Pre-trained language encoders -- functions that represent text as vectors -- are an integral component of many NLP tasks.
                    We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar? We contend that
                    a faithful measure of similarity needs to be <em>intrinsic</em>, that is, task-independent, yet still be informative of <em>extrinsic</em>
                     similarity -- the performance on downstream tasks. It is common to consider two encoders similar if they are <em>homotopic</em>, i.e.,
                     if they can be aligned through some transformation. In this spirit, we study the properties of <em>affine</em> alignment of language
                     encoders and its implications on extrinsic similarity. We find that while affine alignment is fundamentally an asymmetric notion of
                     similarity, it is still informative of extrinsic similarity. We confirm this on datasets of natural language representations. Beyond
                     providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the
                     space of pre-trained encoders by defining an order over them.
                  </small>
                </div>
              </div>
  
              <p style="text-align: center;"> <strong class="small-font">Language Encoders </strong>&#183;<strong class="small-font"> Metric Spaces </strong>&#183;<strong class="small-font"> Large Language Models</strong></p>
            </td>
          </tr>
      </tbody>
          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/api-integration.png' class="publication-img" width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Adapting LLMs for Structured Natural Language API Integration</papertitle>
              <br>
              <u class="small-font">Robin SM Chan</u>, 
              <small>Katsiaryna Mirylenka</small>,
              <small>Thomas Gschwind</small>,
              <small>Christoph Miksovic-Czasch</small>,
              <small>Paolo Scotton</small>,
              <small>Enrico Toniato</small>,
              <small>Abdel Labbi</small>
              <br>
              <em class="small-font">Proceedings of EMNLP: Industry Track</em><span class="small-font">, 2024 | </span> <a class="small-font" href="https://aclanthology.org/2024.emnlp-industry.74.pdf">pdf</a></span>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    Integrating APIs is crucial for enterprise systems, enabling seamless application interaction within workflows.
                    However, the vast and diverse API landscape makes combining calls based on user intent a significant challenge.
                    Existing methods rely on Named Entity Recognition (NER) and knowledge graphs, but struggle with control flow structures like
                    conditionals and loops. We propose a novel framework that leverages the success of Large Language Models (LLMs)
                    in code generation for natural language API integration. Our approach involves fine-tuning an LLM on automatically generated
                    API flows derived from services' OpenAPI specifications. This aims to surpass NER-based methods and compare the effectiveness
                    of different tuning strategies. Specifically, we investigate the impact of enforcing syntax through constrained generation or
                    retrieval-augmented generation. To facilitate systematic comparison, we introduce targeted test suites that assess the generalization
                    capabilities and ability of these approaches to retain structured knowledge. We expect to observe that fine-tuned LLMs can: (a) learn
                    structural constraints implicitly during training, and (b) achieve significant improvements in both in-distribution and out-of-distribution
                    performance.
                  </small>
                </div>
              </div>
              <p style="text-align: center;"> <strong class="small-font">Large Language Models </strong>&#183;<strong class="small-font"> Training </strong>&#183;<strong class="small-font"> API Integration</strong></p>
            </td>
          </tr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/nadav.png' width="100%" class="publication-img" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages</papertitle>
              <br>
              <small>Nadav Borenstein</small>,
              <small>Anej Svete</small>,
              <u class="small-font">Robin SM Chan</u>, 
              <small>Josef Valvoda</small>,
              <small>Franz Nowak</small>,
              <small>Isabelle Augenstein</small>,
              <small>Eleanor Chodroff</small>,
              <small>Ryan Cotterell</small>
              <br>
              <em class="small-font">Proceedings of ACL,</em> <span class="small-font"> 2024 |</span> <a class="small-font" href="https://aclanthology.org/2024.acl-long.807.pdf">pdf</a></span>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    What can large language models learn? By definition, language models (LM) are distributions
                    over strings. Therefore, an intuitive way of
                    addressing the above question is to formalize
                    it as a matter of learnability of <em>classes</em> of distributions over strings. While prior work in this
                    direction focused on assessing the theoretical
                    limits, in contrast, we seek to understand the
                    empirical learnability. Unlike prior empirical
                    work, we evaluate neural LMs on their home
                    turfâ€”learning probabilistic languagesâ€”rather
                    than as classifiers of formal languages. In
                    particular, we investigate the learnability of
                    regular LMs (RLMs) by RNN and Transformer
                    LMs. We empirically test the learnability of
                    RLMs as a function of various complexity
                    parameters of the RLM and the hidden state
                    size of the neural LM. We find that the RLM
                    rank, which corresponds to the size of linear
                    space spanned by the logits of its conditional
                    distributions, and the expected length of
                    sampled strings are strong and significant
                    predictors of learnability for both RNNs and
                    Transformers. Several other predictors also
                    reach significance, but with differing patterns
                    between RNNs and Transformers.
              </small>
            </div>
          </div>
  
              <p style="text-align: center;"> <strong class="small-font">Large Language Models </strong>&#183;<strong class="small-font"> Formal Language Theory</strong></p>
            </td>
          </tr>
      </tbody>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                <img src='images/rnn_website.png' width="100%" class="publication-img" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://arxiv.org/pdf/2402.15814.pdf"> -->
                <papertitle>On Efficiently Representing Regular Languages as RNNs</papertitle>
              </a>
              <br>
              <small>Anej Svete</small>,
              <u class="small-font">Robin SM Chan</u>,
              <small>Ryan Cotterell</small>
              <br>
              <em class="small-font">Findings of ACL</em><span class="small-font">, 2024 | </span> <a class="small-font" href="https://aclanthology.org/2024.findings-acl.244.pdf">pdf</a>
              <br>
              <!-- <br>
              <a class="small-font" href="https://arxiv.org/pdf/2402.15814.pdf">arXiv</a><span class="small-font"></span>
              <br> -->
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                </div>
                <div class="abstract-content">
                  <small>
                    Recent work by <a href="https://aclanthology.org/2020.emnlp-main.156/" class="small-font">Hewitt et al. (2020)</a> provides
                    a possible interpretation of the empirical success of recurrent neural networks (RNNs) as
                    language models (LMs). It shows that RNNs
                    can efficiently represent bounded hierarchical structures that are prevalent in human language. This suggests that RNNs' success might
                    be linked to their ability to model hierarchy.
                    However, a closer inspection of <a href="https://aclanthology.org/2020.emnlp-main.156/" class="small-font">Hewitt et al.'s (2020)</a> construction shows that it is not limited to hierarchical LMs, posing the question
                    of what other classes of LMs can be efficiently
                    represented by RNNs. To this end, we generalize their construction to show that RNNs
                    can efficiently represent a larger class of LMs:
                    Those that can be represented by a pushdown
                    automaton with a bounded stack and a generalized stack update function. This is analogous
                    to an automaton that keeps a memory of a fixed
                    number of symbols and updates the memory
                    with a simple update mechanism. Altogether, the efficiency of representing this
                    diverse class of LMs with RNN LMs suggests
                    novel interpretations of their inductive bias.
                  </small>
              </div>
            </div>
              <p style="text-align: center;"> <strong class="small-font">Language Model Expressivity </strong>&#183;<strong class="small-font"> Formal Language Theory</strong></p>
            </td>
          </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bakedsdf_image'>
                  <img src='images/llm-analyzer.png' width="100%" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Interactive Analysis of LLMs using Meaningful Counterfactuals</papertitle>
              </a>
              <br>
              <small>Furui Cheng</small>,
              <small>VilÃ©m Zouhar</small>,
              <u class="small-font">Robin SM Chan</u>,
              <small>Daniel FÃ¼rst</small>,
              <small>Hendrik Strobelt</small>,
              <small>Menna El-Assady</small>
              <br>
              <em class="small-font">Preprint</em><span class="small-font">, 2024 | </span> <a class="small-font" href="https://arxiv.org/pdf/2405.00708">arXiv</a>
              <br>
              <div class="abstract">
                <div class="abstract-title" onclick="toggleAbstract(event)">
                  <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small>
                </div>
                <div class="abstract-content">
                  <small>
                    Counterfactual examples are useful for exploring the decision boundaries of machine learning models and determining
                      feature attributions. How can we apply counterfactual-based methods to analyze and explain LLMs? We identify the following key
                      challenges. First, the generated textual counterfactuals should be meaningful and readable to users and thus can be mentally compared
                      to draw conclusions. Second, to make the solution scalable to long-form text, users should be equipped with tools to create batches of
                      counterfactuals from perturbations at various granularity levels and interactively analyze the results. In this paper, we tackle the above
                      challenges and contribute 1&#41; a novel algorithm for generating batches of complete and meaningful textual counterfactuals by removing
                      and replacing text segments in different granularities, and 2&#41; LLM Analyzer, an interactive visualization tool to help users understand
                      an LLM's behaviors by interactively inspecting and aggregating meaningful counterfactuals. We evaluate the proposed algorithm by
                      the grammatical correctness of its generated counterfactuals using 1,000 samples from medical, legal, finance, education, and news
                      datasets. In our experiments, 97.2% of the counterfactuals are grammatically correct. Through a use case, user studies, and feedback
                      from experts, we demonstrate the usefulness and usability of the proposed interactive visualization tool
                    </small>
                  </small>
              </div>
            </div>
              <p style="text-align: center;"> <strong class="small-font">Counterfactuals </strong>&#183;<strong class="small-font"> Visual Explainability</strong></p>
            </td>
          </tr> -->
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='bakedsdf_image'>
                  <img src='images/dcc_website.png' width="100%" class="publication-img" style="opacity:0.6; border: 2px solid #00000063; border-radius: 5px;">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <!-- <a href="https://aclanthology.org/2023.acl-demo.44/"> -->
                  <papertitle>Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals</papertitle>
                </a>
                <br>
                <u class="small-font">Robin Chan</u>,
                <small>Afra Amini</small>,
                <small>Menna El-Assady</small>
                <br>
                <em class="small-font">Proceedings of ACL: System Demonstrations</em><span class="small-font">, 2023 | </span>
                <a href="https://aclanthology.org/2023.acl-demo.44.pdf" class="small-font">pdf</a><small> | </small>
                <a href="https://dcc-frontend.onrender.com/" class="small-font">blog post</a> 
                <div class="abstract">
                  <div class="abstract-title" onclick="toggleAbstract(event)">
                    <small>Abstract <i id="abstractArrow" class="fas fa-chevron-down arrow"></i></small> <!-- Downward facing arrow icon -->
                  </div>
                <div class="abstract-content">
                  <small>
                    We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions.
                    The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions.
                    Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback.
                    Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories:
                    Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying
                    training data and assessing NLI models' robustness by creating adversarial test suites.
                  </small>
                </div>
                </div>
                <p style="text-align: center;"> <strong class="small-font">Counterfactuals </strong>&#183;<strong class="small-font"> Mixed-Initiative Learning </strong>&#183;<strong class="small-font"> Language Model Biases</strong></p>
              </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Website source</a>. Consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody>
      </table>
  
      </td>
    </tr>
  </table>

      
      </td>
      </tr>


    </table>
  </div>


  <script>
    function toggleAbstract(event) {
        var titleElement = event.currentTarget;
        var contentElement = titleElement.nextElementSibling;
        var arrowElement = titleElement.querySelector('.arrow');

        if (contentElement.style.display === "none" || contentElement.style.display === "") {
            contentElement.style.display = "block";
            arrowElement.className = "fas fa-chevron-up arrow"; // Upward facing arrow icon
        } else {
            contentElement.style.display = "none";
            arrowElement.className = "fas fa-chevron-down arrow"; // Downward facing arrow icon
        }
      }
    window.onload = function() {
    var container = document.getElementById('container');
    container.style.animation = 'fadeOut 0s forwards';
    container.style.animationDelay = '0.2s'; // Add a delay of 0.5 seconds

    // Add animation-finished class to #content after animation ends
    container.addEventListener('animationend', function() {
      var content = document.getElementById('content');
      content.classList.add('animation-finished');
    });
  };
  </script>



</body>
</html>
